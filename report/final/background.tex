\section{Background}
This project aims to investigate how Machine Learning algorithms (e.g. decision trees, random
forests, boosting) compare to more traditional econometric tools such as linear or logistic
regressions, using a real-world dataset \cite{enquete} to study a binary variable with these
various algorithms.

Our objective is twofold:
\begin{enumerate}[nosep]
    \item compare prediction quality between the various algorithms
    \item compare interpretability of the models and recover marginal effects if possible
\end{enumerate}

The project was supervised by \href{http://www.crest.fr/pagesperso.php?user=3045}{Dr. Romain
Aeberhardt} and \href{http://thomas-larrieu.strikingly.com/}{Thomas Larrieu}.


\subsection{Choice of database and methods}
We have downloaded the \textit{Enqu\^ete emploi en continu 2015} database from the INSEE website.
\textit{Enqu\^ete Emploi} is one of the central elements of the statistical system of knowledge of
employment and unemployment. It is the only source for implementing the definition of unemployment
within the meaning of the International Labor Office. The survey provides original data on
occupations, female and youth activity, hours of work, precarious employment and wages. It makes it
possible to better understand the situation of the unemployed as well as changes in the situation
with regard to work: change from school to work, retirement activity, changes in occupation.
\textit{Enqu\^ete Emploi} contains detailed data on training.


\subsection{Explanation on econometrical methods}
\subsubsection{Linear Regression}

Suppose that we have $n$ distinct predictors. Then the multiple linear regression model takes the
form:

\begin{equation}
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon
\end{equation}

where $x_i$ represents the i$^{th}$ predictor and $\beta_i$ quantifies the association between that
variable and the response. We interpret $\beta_i$ as the average effect on $y$ of a one-unit
increase in $x_i$, holding all other predictors fixed.

The regression coefficients $\beta_0, \beta_1, ..., \beta_n$ are unknown, and must be estimated.
Given estimates $\hat{\beta_0}, \hat{\beta_1}, ..., \hat{\beta_n}$, we can make predictions using
the formula:

\begin{equation}
    \hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + ··· + \hat{\beta_n} x_n
\end{equation}

The parameters are estimated using the same least squares approach, that is, we choose $\beta_0,
\beta_1, ..., \beta_n$ to minimize the sum of squared residuals.

Once we have fit the multiple regression model, it is straightforward to predict the response $y$ on
the basis of a set of values for the predictors $x_1, x_2, ..., x_n$. However, there are three sorts
of uncertainty associated with this prediction:

\begin{enumerate}
    \item The coefficient estimates $\hat{\beta_0}, \hat{\beta_1}, ..., \hat{\beta_n}$ are estimates
for $\beta_0, \beta_1, ..., \beta_n$. That is, the least squares plane $\hat{y} = \hat{\beta_0} +
\hat{\beta_1} x_1 + ··· + \hat{\beta_n} x_n$ is only an estimate for the true population regression
plane $f(x) = \beta_0 + \beta_1 x_1 + ··· + \beta_n x_n$. The inaccuracy in the coefficient
estimates is related to the reducible error. We can compute a confidence interval in order to
determine how close $\hat{y}$ will be to $y = f(x)$.

    \item Of course, in practice assuming a linear model for $f(x)$ is almost always an
approximation of reality, so there is an additional source of potentially reducible error which we
call model bias. So when we use a linear model, we are in fact estimating the best linear
approximation to the true surface. However, here we will ignore this discrepancy, and operate as if
the linear model were correct.

    \item Even if we knew $f(x)$ --- that is, even if we knew the true values for $\beta_0, \beta_1,
..., \beta_n$ --- the response value cannot be predicted perfectly because of the random error
$\epsilon$. How much will $y$ vary from $\hat{y}$? We use prediction intervals to answer this
question. Prediction intervals are always wider than confidence intervals, because they incorporate
both the error in the estimate for $f(x)$ (the reducible error) and the uncertainty as to how much
an individual point will differ from the population regression plane (the irreducible error).
\end{enumerate}


\subsubsection{Classification, the logit and probit models}
The linear regression model assumes that the response variable Y is quantitative. But in many
situations, the response variable is instead qualitative. For example, eye color is qualitative,
taking on values blue, brown, or green. Often qualitative variables are referred to as categorical ;
we will use these terms interchangeably. In this chapter, we study approaches for predicting
qualitative responses, a process that is known as classification. Predicting a qualitative response
for an observation can be referred to as classifying that observation, since it involves assigning
the observation to a category, or class. On the other hand, often the methods used for
classification first predict the probability of each of the categories of a qualitative variable, as
the basis for making the classification. In this sense they also behave like regression methods.
There are many possible classification techniques, or classifiers, that one might use to predict a
qualitative response. In this chapter we two of the most widely-used classifiers: logistic and
probit regression.

For an OLS model, we have:

\begin{equation}
    y = x'\beta + \epsilon
\end{equation}

Binary outcome models estimate the probability that $y=1$ as a function of the independent variables:

\begin{equation}
    p = P(y=1 | x) = F(x’\beta)
\end{equation}

There are 3 different models depending on the functional form of $F(x’\beta)$:

\begin{enumerate}
    \item Regression model (linear probability model): $F(x'\beta) = x'\beta$
    \item Logit model: $F(x'\beta)$ is the cdf of the logistic distribution
    \begin{equation}
        F(x'\beta) = \Lambda(x'\beta) = \frac{e^{x'\beta}}{1+e^{x'\beta}}
    \end{equation}
    The predicted probabilities are limited between 0 and 1.
    \item Probit model: $F(x'\beta)$ is the cdf of the standard normal distribution
    \begin{equation}
        F(x'\beta) = \Phi(x'\beta) = \int_{-\infty}^{+\infty} \phi(z)dz
    \end{equation}
    The predicted probabilities are also limited between 0 and 1.
\end{enumerate}

% TODO: graphs of the linear, logit and probit models

Probit and logit models are estimated using the maximum likelihood method. An increase in $x$
increases/decreases the likelihood that $y=1$ (makes that outcome more/less likely). In other words,
an increase in $x$ makes the outcome of $1$ more or less likely. We interpret the sign of the
coefficient but not the magnitude. The magnitude cannot be interpreted using the coefficient because
different models have different scales of coefficients.

Coefficients differ among models because of the functional form of the $F$ function:

$$\beta_{logit} \approx 4\beta_{OLS}$$
$$\beta_{probit} \approx 2.5\beta_{OLS}$$
$$\beta_{logit} \approx 1.6\beta_{logit}$$

% TODO: explain where this comes from

Now let’s look at the marginal effects. For the OLS regression model, the marginal effects are the
coefficients and they do not depend on $x$. For the logit and probit models, the marginal effects
are calculated as:

\begin{equation}
    \frac{\partial p}{\partial x_j} = F'(x'\beta) \times \beta_j
\end{equation}

The marginal effects depend on $x$, so we need to estimate the marginal effects at a specific value
of $x$ (typically the means).

\paragraph{Marginal effects for the logit model}
\begin{equation}
    \frac{\partial p}{\partial x_j} = \beta_j\Lambda(x'\beta)(1-\Lambda(x'\beta)) = \beta_j\frac{e^{x'\beta}}{(1+e^{x'\beta})^2}
\end{equation}

\paragraph{Marginal effects for the probit model}
\begin{equation}
    \frac{\partial p}{\partial x_j} = \beta_j\Phi'(x'\beta) = \beta_j\phi'(x'\beta)
\end{equation}

\paragraph{Interpretation of marginal effects}
\begin{itemize}
    \item An increase in $x$ increases (decreases) the probability that $y=1$ by the marginal effect expressed as a percentage
    \begin{itemize}
        \item For dummy independent variables, the marginal effect is expressed in comparison to the base category ($x=0$).
        \item For continuous independent variables, the marginal effect is expressed for a one-unit change in $x$.
    \end{itemize}
    \item We interpret both the sign and the magnitude of the marginal effects.
    \item The probit and logit models produce almost identical marginal effects.
\end{itemize}

\paragraph{Odds ratios/relative risk for the logit model}
The odds ratio or relative risk is $\frac{p}{1-p}$ and measures the probability that $y=1$ relative to the probability that $y=0$.

$$p = \frac{e^{x'\beta}}{1+e^{x'\beta}}$$
$$\frac{p}{1-p} = e^{x'\beta}$$
$$\ln \frac{p}{1-p} = x'\beta$$

An odds ratio of $2$ means that the outcome $y=1$ is twice as likely as the outcome of $y=0$. Odds ratios are estimated with the logistic model.
