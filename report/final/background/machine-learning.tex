\section{Machine Learning}

% TODO: introduce the section
% TODO: mention that this section relies heavily on ${references}

\subsection{Decision Trees}

Simple decision trees for regression and classification involve segmenting the predictor space into a number of simple regions. Then, in order to make a
prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splits used to segment the space can be summarised in a tree, these approaches are known as \textit{decision tree methods}.

\subsubsection{Regression trees}
The process of building a regression tree typically involves two steps:

\begin{enumerate}
    \item First, we divide the predictor space -- that is, the set of possible values for $x_1, x_2, ...,
x_n$ -- into $J$ distinct and non-overlapping regions $R_1, R_2, ..., R_J$.
    \item For every observation that falls into the region $R_j$, we predict the response value as the mean of the response values for the training observations in $R_j$.
\end{enumerate}

\paragraph{How are the regions $R_1, ..., R_J$ constructed?}
In theory, the regions $R_1, ..., R_J$ could have any shape, but in practice, the predictor space is typically divided into high-dimensional
rectangles, or \textit{boxes} for simplicity and for ease of interpretation of the resulting model. The goal is then to find boxes $R_1, ..., R_J$ that minimise the RSS, given by:

\begin{equation}
    \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\end{equation}

where $\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ box.

While this method may produce good predictions on the training set, it is likely to overfit the data because the resulting tree might be too complex. A
smaller tree with fewer splits (that is, fewer regions $R_1, ..., R_J$) might lead to lower variance and better interpretation at the cost of a little bias.

There are two strategies to make the resulting tree simpler:

\begin{enumerate}
    \item Build the tree only so long as the decrease in the RSS due to each split exceeds some threshold; this method is considered somewhat "short-sighted" as an early below-threshold split might be followed by a very good split, i.e. a split that leads to a large reduction in RSS
    \item Grow a very large tree $T_0$, and then prune it back in order to obtain a simpler subtree
\end{enumerate}

\subsubsection{Classification trees}
The distinction between regression and classification trees is very similar to the one between the linear and logit/probit regressions. As we have seen in the previous section, in a regression tree, the predicted response for an observation is given by the (mean) response of the training observations that belong to the same region.

In contract, the classification tree predicts that each observation belongs to the \textit{most commonly occuring class} among training observations in the region to which it belongs.

Within this setting, the RSS criterion cannot be used for making the binary splits, so the \textit{classification error rate} (i.e. the fraction of the training observations in that region that do not belong to the most common class) can be used instead:

\begin{equation}
    E = 1 - \max_k{\hat{p}_{mk}}
\end{equation}

where $\hat{p}_{mk}$ represents the proportion of training observations in the $m^{th}$ region that are from the $k_{th}$ class.

However, it turns out that classification error is not sufficiently sensitive for tree-growing, and in practice the \textit{Gini index} is preferred:

\begin{equation}
    G = \sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})
\end{equation}

The Gini index is a measure of total variance across the $K$ classes, and takes on a small value if all of the $\hat{p}_{mk}$’s are close to zero or one. As such, it is also sometimes referred to as a measure of \textit{node purity}, where a small value indicates that a node mostly contains observations from a single class.

% TODO: isn't entropy missing here?

\subsubsection{Advantages \& drawbacks}
\begin{itemize}
    \item Trees are very easy to explain to people, even easier than linear regression.
    \item Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches ;
    \item Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).
    \item Trees can easily handle qualitative predictors without the need to create dummy variables.
\end{itemize}

However, trees usually do not have the same level of accuracy as some regression and classification approaches (as we will see later in the \textit{Results} section).


\subsection{Bagging, random forests, boosting}

Simple trees typically suffer from high variance --- that is, the results of a decision tree fit on distinct data sets are likely to be very different. In constrast, methods such as linear regression tend to have low variance. A popular and substantial improvement over simple trees is the aggregation of many such trees, using methods like bagging, random forests, and boosting.

\subsubsection{Bagging}

Bootstrap aggregation, or \textit{bagging}, is a procedure for reducing the variance of a statistical learning method. Recall that given a set of $n$ independent observations $Z_1$, ..., $Z_n$, each with variance $\sigma^2$, the variance of the mean of $Z$ of the observations is given by $\frac{\sigma^2}{n}$. In other words, averaging a set of observations reduces variance.

Hence, a natural way to reduce the variance and increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we calculate $\hat{f}_1(x), \hat{f}_2(x), ..., \hat{f}_B(x)$ using $B$ separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by:

$$\hat{f}_{avg}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^b(x)$$

In practice, we do not have access to multiple training sets, but we can use a procedure called \textit{boostrap} to generate such sets, by taking repeated samples from the (single) training data set. In this approach we generate $B$ different bootstrapped training data sets. We then train our method on the $b^{th}$ bootstrapped training set in order to get $\hat{f}^b(x)$, and finally average all the predictions, to obtain:

$$\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f^*}^b(x)$$

Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.

\subsubsection{Random Forests}
% TODO(P1): rewrite the "Random Forests" section
Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those $m$ predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \approx \sqrt{p}$ --- that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors. In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting. Random forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average $\frac{p-m}{p}$ of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.

\subsubsection{Boosting}
% TODO(P1): rewrite the "Boosting" section
We now discuss boosting, yet another approach for improving the predictions resulting from a decision tree. Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees. Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Notably, each tree is built on a bootstrap data set, independent of the other trees. Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.

Consider first the regression setting. Like bagging, boosting involves combining a large number of decision trees, $\hat{f}_1, ...,  \hat{f}_B$.

\begin{verbatim}
1. Set ˆ f(x) = 0 and ri = yi for all i in the training set.
2. For b = 1, 2, . . . ,B, repeat:
(a) Fit a tree ˆ fb with d splits (d+1 terminal nodes) to the training
data (X, r).
(b) Update ˆ f by adding in a shrunken version of the new tree:
ˆ f(x) ← ˆ f(x) + λ ˆ fb(x).
(c) Update the residuals,
ri ← ri − λ ˆ fb(xi).
3. Output the boosted model,
\end{verbatim}

Output: $\hat{f}(x) = \frac{1}{B} \sum_{b=1}^{B} \lambda \hat{f}^b (x)$

% TODO(P1): fix the "verbatim" part in the "Boosting" section

What is the idea behind this procedure? Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, we slowly improve $\hat{f}$ in areas where it does not perform well. The shrinkage parameter $\lambda$ slows the process down even further, allowing more and different shaped trees to attack the residuals. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. We have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here. Boosting has three tuning parameters:

\begin{enumerate}
    \item The number of trees $B$. Unlike bagging and random forests, boosting can overfit if $B$ is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select $B$.
    \item The shrinkage parameter $\lambda$, a small positive number. This controls the rate at which boosting learns. Typical values are $0.01$ or $0.001$, and the right choice can depend on the problem. Very small $\lambda$ can require using a very large value of $B$ in order to achieve good performance.
    \item The number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. Often $d=1$ works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally, $d$ is the interaction depth, and controls the interaction order of the boosted model, since $d$ splits can involve at most $d$ variables.
\end{enumerate}
